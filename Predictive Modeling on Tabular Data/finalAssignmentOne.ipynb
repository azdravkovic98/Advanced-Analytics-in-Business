{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Advanced Analytics in Business\n",
    "# Jack Heller, Baris Aksoy, Medha Hegde, Aleksandra Zdravkovic, Group 31\n",
    "\n",
    "Here we start assignment 1 of the course.  Our goal is to identify the most likely customers who will leave the bank.  This is done using a variety of features that include information about the accounts of customers and their balances, as well as the activity of their accounts and any loans they may have with the bank and background information on the customer (education, family, etc.).  This data is also provided for the trailing three months so that changes in patterns can be observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Defintion\n",
    "\n",
    "Due to the limited resources of the bank, the 250 clients that are predicted as most likely to churn will be contacted by the bank in an attempt to retain them.  This means that our goal is not to effectively predict which customers will not churn as it is to find which customers will churn.  Being at a large bank, customers will leave every month.  However, we need to make sure the staff's time is used most effectively which means not wasting their time with customers who will not leave.  This means that a measure like accuracy or AUC would not be appropriate.  Instead, we should count the number of true positives in the top 250.  This can be done using precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve, roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import missingno as msno\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders.woe import WOEEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "curDat = pd.read_csv('train_month_3_with_target.csv')\n",
    "mon2Dat = pd.read_csv('train_month_2.csv')\n",
    "mon1Dat = pd.read_csv('train_month_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "We start by investigating what data we have and how it can be transformed into more valuable features.\n",
    "\n",
    "For a full description of the variables please visit: http://seppe.net/aa/assignment1/\n",
    "\n",
    "Immediately we can see many binary variables describing the customer's relationship with the bank.  These will be very useful not on their own but when combined with trailing data.  For example, if a customer was very active and then stopped using the account, this is a bad sign.  Also if a customer has recently paid off a loan or cancelled insurance, they may be preparing to leave.\n",
    "\n",
    "# Baris insert info about balances here\n",
    "\n",
    "Other variables have a different story to tell.  For example, the variables, 'customer_since_bank' is given as a year and month.  This information is very useful as a customer who has been with the bank for significant time is less likely to leave.  However, this needs to be converted to time as customer in either months or years to allow an easier interpretation of this decaying likelihood.\n",
    "\n",
    "We have another set of variables that need to be featurized in another way.  These are categorical variables where some are ordinal and some are not.  For example, occupation codes are not ordinal.  However, some are certainly more likely to churn than others.  This is true of categories such as customer children as well.  We also have some information on the customer's education level.\n",
    "\n",
    "We also have information on the customer's postal codes.  This poses an interesting challenge as some postal codes appear to have a much higher likelihood of churning than others.  However, some post codes have so few customers that it is not completely reliable and creates a very high dimensional problem as there are many post codes in Belgium.  Fortunately, these 4 digit numbers make up larger areas so we are able to easily aggregate these codes into larger areas by only using the first two numbers.  For a visualization of this, see the map of 2 digit postal codes below.\n",
    "\n",
    "<img src=\"2_digit_postcode_belgique.png\" width=\"400\" height=\"340\">\n",
    "https://en.wikipedia.org/wiki/Postal_codes_in_Belgium\n",
    "\n",
    "When investigating the target data, is it important to note that this data is extremely unbalanced.  With only about 3% of the whole dataset being churning clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homebanking_active</th>\n",
       "      <th>has_homebanking</th>\n",
       "      <th>has_insurance_21</th>\n",
       "      <th>has_insurance_23</th>\n",
       "      <th>has_life_insurance_fixed_cap</th>\n",
       "      <th>has_life_insurance_decreasing_cap</th>\n",
       "      <th>has_fire_car_other_insurance</th>\n",
       "      <th>has_personal_loan</th>\n",
       "      <th>has_mortgage_loan</th>\n",
       "      <th>has_current_account</th>\n",
       "      <th>...</th>\n",
       "      <th>bal_savings_account_starter</th>\n",
       "      <th>bal_current_account_starter</th>\n",
       "      <th>visits_distinct_so</th>\n",
       "      <th>visits_distinct_so_areas</th>\n",
       "      <th>customer_gender</th>\n",
       "      <th>customer_postal_code</th>\n",
       "      <th>customer_occupation_code</th>\n",
       "      <th>customer_self_employed</th>\n",
       "      <th>customer_education</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>63697.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>61695.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "      <td>16572.000000</td>\n",
       "      <td>63697.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.215191</td>\n",
       "      <td>0.280939</td>\n",
       "      <td>0.095028</td>\n",
       "      <td>0.009953</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.111779</td>\n",
       "      <td>0.318053</td>\n",
       "      <td>0.041619</td>\n",
       "      <td>0.098089</td>\n",
       "      <td>0.500809</td>\n",
       "      <td>...</td>\n",
       "      <td>57.641176</td>\n",
       "      <td>30.320894</td>\n",
       "      <td>1.230199</td>\n",
       "      <td>1.042608</td>\n",
       "      <td>1.486444</td>\n",
       "      <td>5577.261959</td>\n",
       "      <td>8.773531</td>\n",
       "      <td>0.087021</td>\n",
       "      <td>2.463734</td>\n",
       "      <td>0.030033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.410958</td>\n",
       "      <td>0.449462</td>\n",
       "      <td>0.293256</td>\n",
       "      <td>0.099270</td>\n",
       "      <td>0.052344</td>\n",
       "      <td>0.315097</td>\n",
       "      <td>0.465724</td>\n",
       "      <td>0.199718</td>\n",
       "      <td>0.297438</td>\n",
       "      <td>0.500003</td>\n",
       "      <td>...</td>\n",
       "      <td>892.959859</td>\n",
       "      <td>407.877892</td>\n",
       "      <td>0.501498</td>\n",
       "      <td>0.224991</td>\n",
       "      <td>0.499820</td>\n",
       "      <td>3020.064554</td>\n",
       "      <td>1.131453</td>\n",
       "      <td>0.281869</td>\n",
       "      <td>1.520309</td>\n",
       "      <td>0.170679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-330.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2650.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4877.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8750.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24050.000000</td>\n",
       "      <td>19790.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9992.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       homebanking_active  has_homebanking  has_insurance_21  \\\n",
       "count        63697.000000     63697.000000      63697.000000   \n",
       "mean             0.215191         0.280939          0.095028   \n",
       "std              0.410958         0.449462          0.293256   \n",
       "min              0.000000         0.000000          0.000000   \n",
       "25%              0.000000         0.000000          0.000000   \n",
       "50%              0.000000         0.000000          0.000000   \n",
       "75%              0.000000         1.000000          0.000000   \n",
       "max              1.000000         1.000000          1.000000   \n",
       "\n",
       "       has_insurance_23  has_life_insurance_fixed_cap  \\\n",
       "count      63697.000000                  63697.000000   \n",
       "mean           0.009953                      0.002747   \n",
       "std            0.099270                      0.052344   \n",
       "min            0.000000                      0.000000   \n",
       "25%            0.000000                      0.000000   \n",
       "50%            0.000000                      0.000000   \n",
       "75%            0.000000                      0.000000   \n",
       "max            1.000000                      1.000000   \n",
       "\n",
       "       has_life_insurance_decreasing_cap  has_fire_car_other_insurance  \\\n",
       "count                       63697.000000                  63697.000000   \n",
       "mean                            0.111779                      0.318053   \n",
       "std                             0.315097                      0.465724   \n",
       "min                             0.000000                      0.000000   \n",
       "25%                             0.000000                      0.000000   \n",
       "50%                             0.000000                      0.000000   \n",
       "75%                             0.000000                      1.000000   \n",
       "max                             1.000000                      1.000000   \n",
       "\n",
       "       has_personal_loan  has_mortgage_loan  has_current_account  ...  \\\n",
       "count       63697.000000       63697.000000         63697.000000  ...   \n",
       "mean            0.041619           0.098089             0.500809  ...   \n",
       "std             0.199718           0.297438             0.500003  ...   \n",
       "min             0.000000           0.000000             0.000000  ...   \n",
       "25%             0.000000           0.000000             0.000000  ...   \n",
       "50%             0.000000           0.000000             1.000000  ...   \n",
       "75%             0.000000           0.000000             1.000000  ...   \n",
       "max             1.000000           1.000000             1.000000  ...   \n",
       "\n",
       "       bal_savings_account_starter  bal_current_account_starter  \\\n",
       "count                 63697.000000                 63697.000000   \n",
       "mean                     57.641176                    30.320894   \n",
       "std                     892.959859                   407.877892   \n",
       "min                       0.000000                  -330.000000   \n",
       "25%                       0.000000                     0.000000   \n",
       "50%                       0.000000                     0.000000   \n",
       "75%                       0.000000                     0.000000   \n",
       "max                   24050.000000                 19790.000000   \n",
       "\n",
       "       visits_distinct_so  visits_distinct_so_areas  customer_gender  \\\n",
       "count        63697.000000              63697.000000     63697.000000   \n",
       "mean             1.230199                  1.042608         1.486444   \n",
       "std              0.501498                  0.224991         0.499820   \n",
       "min              1.000000                  1.000000         1.000000   \n",
       "25%              1.000000                  1.000000         1.000000   \n",
       "50%              1.000000                  1.000000         1.000000   \n",
       "75%              1.000000                  1.000000         2.000000   \n",
       "max              7.000000                  6.000000         2.000000   \n",
       "\n",
       "       customer_postal_code  customer_occupation_code  customer_self_employed  \\\n",
       "count          63697.000000              61695.000000            63697.000000   \n",
       "mean            5577.261959                  8.773531                0.087021   \n",
       "std             3020.064554                  1.131453                0.281869   \n",
       "min                0.000000                  0.000000                0.000000   \n",
       "25%             2650.000000                  9.000000                0.000000   \n",
       "50%             4877.000000                  9.000000                0.000000   \n",
       "75%             8750.000000                  9.000000                0.000000   \n",
       "max             9992.000000                  9.000000                1.000000   \n",
       "\n",
       "       customer_education        target  \n",
       "count        16572.000000  63697.000000  \n",
       "mean             2.463734      0.030033  \n",
       "std              1.520309      0.170679  \n",
       "min              0.000000      0.000000  \n",
       "25%              2.000000      0.000000  \n",
       "50%              2.000000      0.000000  \n",
       "75%              3.000000      0.000000  \n",
       "max              6.000000      1.000000  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curDat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['client_id', 'homebanking_active', 'has_homebanking',\n",
       "       'has_insurance_21', 'has_insurance_23', 'has_life_insurance_fixed_cap',\n",
       "       'has_life_insurance_decreasing_cap', 'has_fire_car_other_insurance',\n",
       "       'has_personal_loan', 'has_mortgage_loan', 'has_current_account',\n",
       "       'has_pension_saving', 'has_savings_account',\n",
       "       'has_savings_account_starter', 'has_current_account_starter',\n",
       "       'bal_insurance_21', 'bal_insurance_23', 'cap_life_insurance_fixed_cap',\n",
       "       'cap_life_insurance_decreasing_cap', 'prem_fire_car_other_insurance',\n",
       "       'bal_personal_loan', 'bal_mortgage_loan', 'bal_current_account',\n",
       "       'bal_pension_saving', 'bal_savings_account',\n",
       "       'bal_savings_account_starter', 'bal_current_account_starter',\n",
       "       'visits_distinct_so', 'visits_distinct_so_areas', 'customer_since_all',\n",
       "       'customer_since_bank', 'customer_gender', 'customer_birth_date',\n",
       "       'customer_postal_code', 'customer_occupation_code',\n",
       "       'customer_self_employed', 'customer_education', 'customer_children',\n",
       "       'customer_relationship', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curDat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Completeness\n",
    "\n",
    "One challenge we did have is that not all customers fill in every field and we have incomplete information about every customer.  Removing these customers from the dataset is not a good option as this will continue to be a problem in the test set.  However, we can investigate how these missing data points impact their likelihood to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "client_id                                0\n",
       "homebanking_active                       0\n",
       "has_homebanking                          0\n",
       "has_insurance_21                         0\n",
       "has_insurance_23                         0\n",
       "has_life_insurance_fixed_cap             0\n",
       "has_life_insurance_decreasing_cap        0\n",
       "has_fire_car_other_insurance             0\n",
       "has_personal_loan                        0\n",
       "has_mortgage_loan                        0\n",
       "has_current_account                      0\n",
       "has_pension_saving                       0\n",
       "has_savings_account                      0\n",
       "has_savings_account_starter              0\n",
       "has_current_account_starter              0\n",
       "bal_insurance_21                         0\n",
       "bal_insurance_23                         0\n",
       "cap_life_insurance_fixed_cap             0\n",
       "cap_life_insurance_decreasing_cap        0\n",
       "prem_fire_car_other_insurance            0\n",
       "bal_personal_loan                        0\n",
       "bal_mortgage_loan                        0\n",
       "bal_current_account                      0\n",
       "bal_pension_saving                       0\n",
       "bal_savings_account                      0\n",
       "bal_savings_account_starter              0\n",
       "bal_current_account_starter              0\n",
       "visits_distinct_so                       0\n",
       "visits_distinct_so_areas                 0\n",
       "customer_since_all                     234\n",
       "customer_since_bank                    249\n",
       "customer_gender                          0\n",
       "customer_birth_date                      0\n",
       "customer_postal_code                     0\n",
       "customer_occupation_code              2002\n",
       "customer_self_employed                   0\n",
       "customer_education                   47125\n",
       "customer_children                    23364\n",
       "customer_relationship                14899\n",
       "target                                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curDat.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by evaluating how a customers lack of information effects their likelihood of churning.  We can see that while churning is pretty similar among all education levels with the exception of 6, customers who did not fill in their information are much less likely to churn.  We can also see that the vast majority of customers did not include their education level information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  customer_education    target\n",
      "0                0.0  0.057851\n",
      "1                1.0  0.066593\n",
      "2                2.0  0.063693\n",
      "3                3.0  0.062213\n",
      "4                4.0  0.057471\n",
      "5                5.0  0.059593\n",
      "6                6.0  0.032154\n",
      "7                nan  0.018992\n",
      "  customer_education  target\n",
      "0                0.0    2178\n",
      "1                1.0    1802\n",
      "2                2.0    4506\n",
      "3                3.0    5015\n",
      "4                4.0     696\n",
      "5                5.0    2064\n",
      "6                6.0     311\n",
      "7                nan   47125\n"
     ]
    }
   ],
   "source": [
    "# Can be seen that customer education na is a big effect\n",
    "curDat['customer_education'] = curDat['customer_education'].fillna('nan')\n",
    "print(curDat.groupby('customer_education', as_index=False)['target'].mean())\n",
    "print(curDat.groupby('customer_education', as_index=False)['target'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when we look at occupation codes, almost all of our customers have occupation 9.  While evaluating solely on percentage churning by occupation it shows that some don't churn.  However, with so few observations, it is difficult to say definitively that they really are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_occupation_code    target\n",
      "0                       0.0  0.026128\n",
      "1                       1.0  0.000000\n",
      "2                       2.0  0.000000\n",
      "3                       3.0  0.100000\n",
      "4                       4.0  0.041489\n",
      "5                       5.0  0.071895\n",
      "6                       6.0  0.054645\n",
      "7                       7.0  0.038462\n",
      "8                       8.0  0.037736\n",
      "9                       9.0  0.029217\n",
      "10                      nan  0.038462\n",
      "   customer_occupation_code  target\n",
      "0                       0.0     421\n",
      "1                       1.0      24\n",
      "2                       2.0       7\n",
      "3                       3.0      10\n",
      "4                       4.0    1639\n",
      "5                       5.0     153\n",
      "6                       6.0     183\n",
      "7                       7.0     104\n",
      "8                       8.0     318\n",
      "9                       9.0   58836\n",
      "10                      nan    2002\n"
     ]
    }
   ],
   "source": [
    "curDat['customer_occupation_code'] = curDat['customer_occupation_code'].fillna('nan')\n",
    "print(curDat.groupby('customer_occupation_code', as_index=False)['target'].mean())\n",
    "print(curDat.groupby('customer_occupation_code', as_index=False)['target'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing the same evaluation for children, we see that customers with one baby, preschool children, young children, and those who simply answered yes are much more likely to churn.  They are also a small population compared to the overall population.  On the other side, those who did not answer or answered no make up almost all of the customers and have very similar likelihoods of churning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  customer_children    target\n",
      "0        adolescent  0.032464\n",
      "1           grownup  0.035115\n",
      "2            mature  0.025366\n",
      "3               nan  0.028805\n",
      "4                no  0.023202\n",
      "5           onebaby  0.057981\n",
      "6         preschool  0.063738\n",
      "7               yes  0.065089\n",
      "8             young  0.051659\n",
      "  customer_children  target\n",
      "0        adolescent    3912\n",
      "1           grownup    1908\n",
      "2            mature    4849\n",
      "3               nan   23364\n",
      "4                no   22886\n",
      "5           onebaby    1466\n",
      "6         preschool    2322\n",
      "7               yes     338\n",
      "8             young    2652\n"
     ]
    }
   ],
   "source": [
    "curDat['customer_children'] = curDat['customer_children'].fillna('nan')\n",
    "print(curDat.groupby('customer_children', as_index=False)['target'].mean())\n",
    "print(curDat.groupby('customer_children', as_index=False)['target'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do one more analysis in this fashion on customer relationships. Here we see that being in a relationship appears to have very little effect on if they will churn and not answering is also not a strong indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  customer_relationship    target\n",
      "0                couple  0.031317\n",
      "1                   nan  0.029935\n",
      "2                single  0.026468\n",
      "  customer_relationship  target\n",
      "0                couple   36179\n",
      "1                   nan   14899\n",
      "2                single   12619\n"
     ]
    }
   ],
   "source": [
    "curDat['customer_relationship'] = curDat['customer_relationship'].fillna('nan')\n",
    "print(curDat.groupby('customer_relationship', as_index=False)['target'].mean())\n",
    "print(curDat.groupby('customer_relationship', as_index=False)['target'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why did we drop postal code?\n",
    "\n",
    "Here we featurize the data as described.  We:\n",
    "- Remove Client ID\n",
    "- Create a feature to identify if they were homebanking 3 months ago and stopped\n",
    "- If their savings account balance changed\n",
    "- If the customer has a current checking account\n",
    "- If there was a change in the customer's loan or mortgage status\n",
    "- Postal code is dropped as it will be added back in later\n",
    "- We use a median imputation for how long the customer has been with the bank as this data is rarely NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "curDat = pd.read_csv('train_month_3_with_target.csv')\n",
    "\n",
    "curDat = curDat.drop(['client_id'], axis = 1)\n",
    "\n",
    "homeChange = curDat['homebanking_active'] - mon1Dat['homebanking_active']\n",
    "curDat.insert(1, \"homebank_change\", homeChange)\n",
    "\n",
    "saveBalChange = curDat['bal_savings_account'] - mon1Dat['bal_savings_account']\n",
    "curDat.insert(1, \"saveBalChange\", saveBalChange)\n",
    "\n",
    "curActChange = curDat['has_current_account'] - mon1Dat['has_current_account']\n",
    "curDat.insert(1, \"has_current_account_change\", curActChange)\n",
    "\n",
    "perLoanChange = mon1Dat['has_personal_loan'] - curDat['has_personal_loan']\n",
    "curDat.insert(1, 'change_personal_loan', perLoanChange)\n",
    "\n",
    "homeLoanChange = mon1Dat['has_mortgage_loan'] - curDat['has_mortgage_loan']\n",
    "curDat.insert(1, 'change_mortgage_loan', homeLoanChange)\n",
    "\n",
    "curDat['customer_occupation_code'] = curDat['customer_occupation_code'].fillna(10)\n",
    "dumOccCode = pd.get_dummies(curDat['customer_occupation_code'], prefix = 'occ_code')\n",
    "curDat = pd.concat([dumOccCode, curDat], axis = 1)\n",
    "curDat = curDat.drop(['customer_occupation_code'], axis = 1)\n",
    "\n",
    "curDat['customer_education'] = curDat['customer_education'].fillna(7)\n",
    "\n",
    "curDat['customer_children'] = curDat['customer_children'].fillna('nan')\n",
    "dumChilCode = pd.get_dummies(curDat['customer_children'], prefix = 'child_code')\n",
    "curDat = pd.concat([dumChilCode, curDat], axis = 1)\n",
    "curDat = curDat.drop(['customer_children'], axis = 1)\n",
    "\n",
    "curDat = curDat.drop(['customer_relationship'], axis = 1)\n",
    "\n",
    "curDat['customer_since_all'] = (datetime.datetime.today() - pd.to_datetime(curDat['customer_since_all'])).dt.days\n",
    "curDat['customer_since_bank'] = (datetime.datetime.today() - pd.to_datetime(curDat['customer_since_bank'])).dt.days\n",
    "curDat['customer_birth_date'] = (datetime.datetime.today() - pd.to_datetime(curDat['customer_birth_date'])).dt.days\n",
    "\n",
    "curDat['customer_since_all'] = curDat['customer_since_all'].fillna(curDat['customer_since_all'].median())\n",
    "curDat['customer_since_bank'] = curDat['customer_since_bank'].fillna(curDat['customer_since_bank'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "\n",
    "We next work to deal with the class imbalance problem.  We will do this by using random oversampling.  We do this at this point as to make sure we have a validation set to evaluate on for tuning and we only create observation training reliant features on the training set.  We found that oversampling was a key step in this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shouldn't we do the train test split before EDA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_train = curDat.iloc[:,:-1]\n",
    "y_train = curDat.iloc[:,-1]\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(X_train, y_train, \n",
    "    test_size=0.25, random_state= 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Code Featurization\n",
    "\n",
    "Now that we have a training set, we can featurize this by taking the likelihood of churning by post code and creating a dictionary.  We then replace the post codes with these likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#postDatFrame = pd.concat([Xtrain, ytrain], axis = 1)\n",
    "#meanPost = pd.Series(postDatFrame.groupby('customer_postal_code')['target'].mean())\n",
    "#countPost = postDatFrame.groupby('customer_postal_code')['target'].count()\n",
    "#postDict = pd.Series(meanPost,index=meanPost.index).to_dict()\n",
    "\n",
    "#Xtrain.replace({'customer_postal_code': postDict},inplace=True)\n",
    "#Xval.replace({'customer_postal_code': postDict},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now scale all the training data and perform the same transform on our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(Xtrain)\n",
    "\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "\n",
    "Xval = scaler.transform(Xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "The first model we investigated was a basic logistic regression.  This allows us to set a benchmark for how different models perform and their precision at different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression()\n",
    "logReg.fit(Xtrain, ytrain)\n",
    "predicted = logReg.predict_proba(Xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to find percentage correctly identified in top numberEval predictions\n",
    "def pctCorrectTop(numberEval, true, preds):\n",
    "    idx = (-preds[:,1]).argsort()[:numberEval]\n",
    "    yval2 = yval\n",
    "    yval2 = yval2.reset_index()\n",
    "    return(yval2.iloc[idx,:]['target'].sum() / numberEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.14\n",
      "0.15\n",
      "0.156\n"
     ]
    }
   ],
   "source": [
    "logReg10 = pctCorrectTop(10, yval, predicted)\n",
    "logReg50 = pctCorrectTop(50, yval, predicted)\n",
    "logReg100 = pctCorrectTop(100, yval, predicted)\n",
    "logReg250 = pctCorrectTop(250, yval, predicted)\n",
    "print(logReg10)\n",
    "print(logReg50)\n",
    "print(logReg100)\n",
    "print(logReg250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.1\n",
      "0.08\n",
      "0.124\n"
     ]
    }
   ],
   "source": [
    "rfClass = RandomForestClassifier()\n",
    "rfClass.fit(Xtrain, ytrain)\n",
    "val_predict=rfClass.predict_proba(Xval)\n",
    "rf10 = pctCorrectTop(10, yval, val_predict)\n",
    "rf50 = pctCorrectTop(50, yval, val_predict)\n",
    "rf100 = pctCorrectTop(100, yval, val_predict)\n",
    "rf250 = pctCorrectTop(250, yval, val_predict)\n",
    "print(rf10)\n",
    "print(rf50)\n",
    "print(rf100)\n",
    "print(rf250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.16\n",
      "0.18\n",
      "0.152\n"
     ]
    }
   ],
   "source": [
    "gradBoost = GradientBoostingClassifier()\n",
    "gradBoost.fit(Xtrain, ytrain)\n",
    "gradPreds = gradBoost.predict_proba(Xval)\n",
    "gb10 = pctCorrectTop(10, yval, gradPreds)\n",
    "gb50 = pctCorrectTop(50, yval, gradPreds)\n",
    "gb100 = pctCorrectTop(100, yval, gradPreds)\n",
    "gb250 = pctCorrectTop(250, yval, gradPreds)\n",
    "print(gb10)\n",
    "print(gb50)\n",
    "print(gb100)\n",
    "print(gb250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison and Results\n",
    "\n",
    "Here we can see that at every level, gradient boosting trees performed the best on the validation set.  For this reason, it appears to be the best model to proceed with.  We can also see the precision as various amounts of observations taken.  For gradient boosting, it may be better to take less than the top 250, the precision is much better for the top 100.  Of course, more churners were found in the top 250 but the benefit of pursuing fewer clients is that less resources and investment is required and there is a greater ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>TP 10</th>\n",
       "      <th>TP 50</th>\n",
       "      <th>TP 100</th>\n",
       "      <th>TP 250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting Tree</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model TP 10 TP 50 TP 100 TP 250\n",
       "0     Logistic Regression   0.1  0.14   0.15  0.156\n",
       "1           Random Forest   0.0   0.1   0.08  0.124\n",
       "2  Gradient Boosting Tree   0.2  0.16   0.18  0.152"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npDf = np.array([('Logistic Regression', logReg10, logReg50, logReg100, logReg250),\n",
    "                ('Random Forest', rf10, rf50, rf100, rf250),\n",
    "                ('Gradient Boosting Tree', gb10, gb50, gb100, gb250)])\n",
    "resultsDf = pd.DataFrame(npDf, columns = ['Model', 'TP 10', 'TP 50', 'TP 100', 'TP 250'])\n",
    "resultsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of Test Set Predictions\n",
    "\n",
    "Here we create our test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = pd.read_csv('test_month_3.csv')\n",
    "test1 = pd.read_csv('test_month_1.csv')\n",
    "\n",
    "ids = test3['client_id']\n",
    "test3 = test3.drop(['client_id'], axis = 1)\n",
    "\n",
    "homeChange = test3['homebanking_active'] - test1['homebanking_active']\n",
    "test3.insert(1, \"homebank_change\", homeChange)\n",
    "\n",
    "saveBalChange = test3['bal_savings_account'] - test1['bal_savings_account']\n",
    "test3.insert(1, \"saveBalChange\", saveBalChange)\n",
    "\n",
    "curActChange = test3['has_current_account'] - test1['has_current_account']\n",
    "test3.insert(1, \"has_current_account_change\", curActChange)\n",
    "\n",
    "perLoanChange = test1['has_personal_loan'] - test3['has_personal_loan']\n",
    "test3.insert(1, 'change_personal_loan', perLoanChange)\n",
    "\n",
    "homeLoanChange = test1['has_mortgage_loan'] - test3['has_mortgage_loan']\n",
    "test3.insert(1, 'change_mortgage_loan', homeLoanChange)\n",
    "\n",
    "test3['customer_occupation_code'] = test3['customer_occupation_code'].fillna(10)\n",
    "dumOccCode = pd.get_dummies(test3['customer_occupation_code'], prefix = 'occ_code')\n",
    "test3 = pd.concat([dumOccCode, test3], axis = 1)\n",
    "test3 = test3.drop(['customer_occupation_code'], axis = 1)\n",
    "\n",
    "test3['customer_education'] = test3['customer_education'].fillna(7)\n",
    "\n",
    "test3['customer_children'] = test3['customer_children'].fillna('nan')\n",
    "dumChilCode = pd.get_dummies(test3['customer_children'], prefix = 'child_code')\n",
    "test3 = pd.concat([dumChilCode, test3], axis = 1)\n",
    "test3 = test3.drop(['customer_children'], axis = 1)\n",
    "\n",
    "test3 = test3.drop(['customer_relationship'], axis = 1)\n",
    "\n",
    "test3['customer_since_all'] = (datetime.datetime.today() - pd.to_datetime(test3['customer_since_all'])).dt.days\n",
    "test3['customer_since_bank'] = (datetime.datetime.today() - pd.to_datetime(test3['customer_since_bank'])).dt.days\n",
    "test3['customer_birth_date'] = (datetime.datetime.today() - pd.to_datetime(test3['customer_birth_date'])).dt.days\n",
    "\n",
    "test3['customer_since_all'] = test3['customer_since_all'].fillna(curDat['customer_since_all'].median())\n",
    "test3['customer_since_bank'] = test3['customer_since_bank'].fillna(curDat['customer_since_bank'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = scaler.transform(test3)\n",
    "\n",
    "finalPredictions = pd.Series(gradBoost.predict_proba(xtest)[:,1])\n",
    "\n",
    "fpredict=pd.concat([ids,finalPredictions],axis=1)\n",
    "fpredict.to_csv('test_predictions',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "When we see the second half of the test set, we had 35 true positives out of the top 250.  This is encouraging performance after seeing 36 true positives on the first test set.  We had 39 true positives on the validation set so while we did have some degradation in performance, it was acceptable.  We were very careful to not continuously submit models to the test set but we instead performed tuning and different approaches on the validation set.  This is why we saw a similar performance.\n",
    "\n",
    "While the definition of the target variable is not bad the way it is, it may be helpful to we see from our validation set that the true positive rate is better when only taking the top 100 candidates.  Our recommendation would be that the bank perform a cost benefit analysis to evaluate if the additional resources required to contact 150 more people is worth the payoff of finding the extra people.  This could be done through a breakeven analysis of what is an acceptable true positive rate and then finding a good threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfEnv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
